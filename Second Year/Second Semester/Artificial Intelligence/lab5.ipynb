{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8210b19",
   "metadata": {},
   "source": [
    "## A.I. Assignment 5\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "* Get more familiar with tensors in pytorch \n",
    "* Create a simple multilayer perceptron model with pytorch\n",
    "* Visualise the parameters\n",
    "\n",
    "\n",
    "### Task\n",
    "\n",
    "Build a fully connected feed forward network that adds two bits. Determine the a propper achitecture for this network (what database you use for this problem? how many layers? how many neurons on each layer? what is the activation function? what is the loss function? etc)\n",
    "\n",
    "Create at least 3 such networks and compare their performance (how accurate they are?, how farst they are trained to get at 1 accuracy?)\n",
    "\n",
    "Display for the best one the weights for each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3614e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n",
      "\n",
      "Expected output:\n",
      "tensor([[0., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]])\n",
      "\n",
      "Epoch: 0, Max Loss: 0.27890267968177795, Min Loss: 0.25272318720817566\n",
      "Epoch: 3000, Max Loss: 0.1455816626548767, Min Loss: 0.028080597519874573\n",
      "Epoch: 6000, Max Loss: 0.06974738091230392, Min Loss: 0.002628481946885586\n",
      "Epoch: 9000, Max Loss: 0.023361336439847946, Min Loss: 5.3513701914198464e-08\n",
      "Model: Sequential(\n",
      "  (lin1): Linear(in_features=2, out_features=5, bias=True)\n",
      "  (sigmoid1): Sigmoid()\n",
      "  (lin2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  (sigmoid2): Sigmoid()\n",
      ")\n",
      "Result: [[6.5972335e-03 7.0805600e-06]\n",
      " [9.9146867e-01 8.3521446e-03]\n",
      " [9.9118441e-01 8.3116032e-03]\n",
      " [1.2542469e-02 9.8300201e-01]]\n",
      "Expected result: [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: 9.73885107669048e-05\n",
      "lin1.weight tensor([[-4.9447, -4.9342],\n",
      "        [-4.2293, -4.3709],\n",
      "        [-4.9547, -4.9262],\n",
      "        [-5.2006, -5.2065],\n",
      "        [-4.5331, -4.4151]])\n",
      "lin1.bias tensor([2.0033, 1.9128, 6.8583, 7.2211, 1.8178])\n",
      "lin2.weight tensor([[-4.4548, -4.8208,  6.3240,  5.7091, -4.6916],\n",
      "        [-1.8232, -3.2224, -5.0061, -4.9798, -2.2793]])\n",
      "lin2.bias tensor([-4.8747,  4.4941])\n",
      "\n",
      "\n",
      "Model: Sequential(\n",
      "  (lin1): Linear(in_features=2, out_features=5, bias=True)\n",
      "  (sigmoid1): Hardsigmoid()\n",
      "  (lin2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  (sigmoid2): Hardsigmoid()\n",
      ")\n",
      "Result: [[0.11370122 0.        ]\n",
      " [0.88625604 0.        ]\n",
      " [0.88625294 0.        ]\n",
      " [0.11383887 1.        ]]\n",
      "Expected result: [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: 0.006470417138189077\n",
      "lin1.weight tensor([[-2.5877, -2.4547],\n",
      "        [-2.0281, -1.9824],\n",
      "        [-6.0057, -6.0058],\n",
      "        [-2.6238, -3.0623],\n",
      "        [-2.8343, -2.6037]])\n",
      "lin1.bias tensor([2.0552, 1.0186, 3.0049, 2.6987, 2.4427])\n",
      "lin2.weight tensor([[ 3.1475,  2.6278, -9.2698,  2.6143,  2.6368],\n",
      "        [-3.6554, -3.2838, -2.8949, -3.8564, -3.4796]])\n",
      "lin2.bias tensor([-2.3347,  3.0386])\n",
      "\n",
      "\n",
      "Model: Sequential(\n",
      "  (lin1): Linear(in_features=2, out_features=7, bias=True)\n",
      "  (sigmoid1): Hardsigmoid()\n",
      "  (lin2): Linear(in_features=7, out_features=2, bias=True)\n",
      "  (sigmoid2): Hardsigmoid()\n",
      ")\n",
      "Result: [[7.5220190e-02 0.0000000e+00]\n",
      " [9.2477489e-01 2.9404957e-06]\n",
      " [9.2477107e-01 0.0000000e+00]\n",
      " [7.5235128e-02 1.0000000e+00]]\n",
      "Expected result: [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: 0.002829576376825571\n",
      "lin1.weight tensor([[-2.8605, -2.3621],\n",
      "        [-2.3733, -2.6162],\n",
      "        [ 2.5147,  2.7917],\n",
      "        [-2.5341, -2.5251],\n",
      "        [-2.0436, -2.7728],\n",
      "        [-3.0629, -2.1458],\n",
      "        [-6.0063, -6.0066]])\n",
      "lin1.bias tensor([ 2.2523,  1.9943, -2.3258,  2.0857,  1.8278,  2.2252,  3.0045])\n",
      "lin2.weight tensor([[  2.2289,   1.8092,  -2.1861,   2.0227,   2.1369,   1.6076, -10.1945],\n",
      "        [ -2.3216,  -2.6946,   2.7479,  -2.2547,  -2.1657,  -1.9782,  -1.5490]])\n",
      "lin2.bias tensor([-0.3994,  0.3183])\n",
      "\n",
      "\n",
      "Model: Sequential(\n",
      "  (lin1): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (sigmoid1): Hardsigmoid()\n",
      "  (lin2): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (sigmoid2): Hardsigmoid()\n",
      ")\n",
      "Result: [[0.04317363 0.        ]\n",
      " [0.9568179  0.        ]\n",
      " [0.9568153  0.        ]\n",
      " [0.04321365 1.        ]]\n",
      "Expected result: [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: 0.00093262386508286\n",
      "lin1.weight tensor([[-1.4769, -2.5729],\n",
      "        [-2.1391, -2.2587],\n",
      "        [-1.7661, -1.9417],\n",
      "        [-6.0049, -6.0054],\n",
      "        [-1.6402, -1.8021],\n",
      "        [-2.1244, -2.1202],\n",
      "        [-1.8576, -1.6240],\n",
      "        [-2.0533, -1.4262],\n",
      "        [-2.8223, -1.6760],\n",
      "        [-1.6263, -1.9426]])\n",
      "lin1.bias tensor([1.0519, 1.4153, 0.7266, 3.0038, 0.4507, 1.2677, 0.4982, 0.4895, 1.4991,\n",
      "        0.5782])\n",
      "lin2.weight tensor([[  1.8749,   1.8996,   2.2837, -10.9635,   1.6410,   1.7566,   2.1632,\n",
      "           1.7436,   1.7101,   1.9528],\n",
      "        [ -2.2510,  -2.2118,  -2.1184,  -0.9387,  -2.0226,  -2.0622,  -1.7476,\n",
      "          -1.9627,  -2.0874,  -2.0903]])\n",
      "lin2.bias tensor([-2.7753,  3.0414])\n",
      "\n",
      "\n",
      "Model: Sequential(\n",
      "  (lin1): Linear(in_features=2, out_features=15, bias=True)\n",
      "  (sigmoid1): Hardsigmoid()\n",
      "  (lin2): Linear(in_features=15, out_features=2, bias=True)\n",
      "  (sigmoid2): Hardsigmoid()\n",
      ")\n",
      "Result: [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Expected result: [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: 0.0\n",
      "lin1.weight tensor([[-1.7191, -1.8881],\n",
      "        [-5.6888, -5.6888],\n",
      "        [-2.1125, -1.7155],\n",
      "        [-2.1348, -1.9853],\n",
      "        [-5.7006, -5.7006],\n",
      "        [-1.8805, -2.4325],\n",
      "        [ 1.9366,  1.9270],\n",
      "        [-1.8207, -2.1354],\n",
      "        [-2.8546, -2.1732],\n",
      "        [-2.2422, -1.7327],\n",
      "        [ 1.2538,  2.4172],\n",
      "        [-2.7357, -2.1329],\n",
      "        [-2.3412, -1.5236],\n",
      "        [-2.2084, -1.7861],\n",
      "        [-1.3543, -2.4866]])\n",
      "lin1.bias tensor([ 0.6086,  2.6888,  0.8307,  1.1207,  2.7006,  1.3152, -0.8663,  0.9585,\n",
      "         2.0300,  0.9751, -0.6733,  1.8710,  0.8654,  0.9964,  0.8429])\n",
      "lin2.weight tensor([[ 1.4579, -6.1958,  1.3952,  1.6088, -6.4482,  1.5858, -1.3917,  1.1371,\n",
      "          1.1744,  1.3391, -1.3082,  1.2281,  1.2451,  1.4168,  1.4531],\n",
      "        [-1.2960, -0.8181, -1.4966, -1.2847, -1.1875, -1.5333,  1.5117, -1.4102,\n",
      "         -1.4177, -1.1885,  1.2912, -1.3677, -0.9914, -1.4170, -1.5050]])\n",
      "lin2.bias tensor([-0.3056,  0.2145])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# Input data.\n",
    "data_in = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]]).float()\n",
    "print(\"Input data:\\n\" + str(data_in) +\"\\n\")\n",
    "\n",
    "# Expected data.\n",
    "data_target = torch.tensor([[0, 0], [1, 0], [1, 0], [0, 1]]).float()\n",
    "print(\"Expected output:\\n\" + str(data_target) + \"\\n\")\n",
    "\n",
    "# The models are here.\n",
    "models = [\n",
    "    nn.Sequential(OrderedDict([\n",
    "        ('lin1', nn.Linear(2, 5)),\n",
    "        ('sigmoid1', nn.Sigmoid()),\n",
    "        ('lin2', nn.Linear(5, 2)),\n",
    "        ('sigmoid2', nn.Sigmoid())\n",
    "    ])),\n",
    "    nn.Sequential(OrderedDict([\n",
    "        ('lin1', nn.Linear(2, 5)),\n",
    "        ('sigmoid1', nn.Hardsigmoid()),\n",
    "        ('lin2', nn.Linear(5, 2)),\n",
    "        ('sigmoid2', nn.Hardsigmoid())   # This has better results than Sigmoid.\n",
    "    ])),\n",
    "    nn.Sequential(OrderedDict([\n",
    "        ('lin1', nn.Linear(2, 7)),\n",
    "        ('sigmoid1', nn.Hardsigmoid()),\n",
    "        ('lin2', nn.Linear(7, 2)),\n",
    "        ('sigmoid2', nn.Hardsigmoid())\n",
    "    ])),\n",
    "    nn.Sequential(OrderedDict([\n",
    "        ('lin1', nn.Linear(2, 10)),\n",
    "        ('sigmoid1', nn.Hardsigmoid()),\n",
    "        ('lin2', nn.Linear(10, 2)),\n",
    "        ('sigmoid2', nn.Hardsigmoid())\n",
    "    ])),\n",
    "    nn.Sequential(OrderedDict([\n",
    "        ('lin1', nn.Linear(2, 15)),\n",
    "        ('sigmoid1', nn.Hardsigmoid()),\n",
    "        ('lin2', nn.Linear(15, 2)),\n",
    "        ('sigmoid2', nn.Hardsigmoid())\n",
    "    ])),\n",
    "]\n",
    "\n",
    "# Optimizers and stuff.\n",
    "criterion = nn.MSELoss()                                                # MSE Loss\n",
    "optimizers = []                                                         # List of optimizers\n",
    "for model in models:\n",
    "    optimizers.append(torch.optim.Adam(model.parameters()))\n",
    "\n",
    "# Train the models\n",
    "for epoch in range(12000):\n",
    "    maxLoss = 0\n",
    "    minLoss = 1\n",
    "    for model, optimizer in zip(models, optimizers):\n",
    "        output = model(data_in)\n",
    "        loss = criterion(output, data_target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        maxLoss = max(maxLoss, loss.item())\n",
    "        minLoss = min(minLoss, loss.item())\n",
    "    if epoch % 3000 == 0:\n",
    "        print(\"Epoch: \" + str(epoch) + \", Max Loss: \" + str(maxLoss) + \", Min Loss: \" + str(minLoss))\n",
    "\n",
    "# Print model weights and results.\n",
    "for model in models:\n",
    "    result = model(data_in)\n",
    "    loss = criterion(result, data_target)\n",
    "    print(\"Model: \" + str(model))\n",
    "    print(\"Result: \" + str(result.detach().numpy()))\n",
    "    print(\"Expected result: \" + str(data_target.numpy()))\n",
    "    print(\"Loss: \" + str(loss.item()))\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.data)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
