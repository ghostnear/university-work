{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8210b19",
   "metadata": {},
   "source": [
    "## A.I. Assignment 5\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "* Get more familiar with tensors in pytorch \n",
    "* Create a simple multilayer perceptron model with pytorch\n",
    "* Visualise the parameters\n",
    "\n",
    "\n",
    "### Task\n",
    "\n",
    "Build a fully connected feed forward network that adds two bits. Determine the a propper achitecture for this network (what database you use for this problem? how many layers? how many neurons on each layer? what is the activation function? what is the loss function? etc)\n",
    "\n",
    "Create at least 3 such networks and compare their performance (how accurate they are?, how farst they are trained to get at 1 accuracy?)\n",
    "\n",
    "Display for the best one the weights for each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e3614e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n",
      "\n",
      "Expected output:\n",
      "tensor([[0., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]])\n",
      "\n",
      "Epoch: 10, Max Loss: 0.1602407544851303, Min Loss: 0.0034742618445307016\n",
      "The best model ended training. It took 12 epochs.\n",
      "\n",
      "Model: Sequential(\n",
      "  (lin1): Linear(in_features=2, out_features=5, bias=True)\n",
      "  (relu1): LeakyReLU(negative_slope=0.01)\n",
      "  (lin2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  (sigmoid): Hardsigmoid()\n",
      ")\n",
      "Result: [[0.6564112  0.        ]\n",
      " [0.64454406 0.        ]\n",
      " [0.6391348  0.        ]\n",
      " [0.         1.        ]]\n",
      "Expected result: [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: 0.08593103289604187\n",
      "Accuracy: 0.9140689671039581\n",
      "lin1.weight tensor([[ 1.3893,  1.3606],\n",
      "        [-2.6761, -2.5410],\n",
      "        [-2.1244, -1.9792],\n",
      "        [ 2.2022,  2.1130],\n",
      "        [-1.4865, -1.8270]])\n",
      "lin1.bias tensor([-1.3292, -1.8262, -2.0961, -2.3019, -3.3766])\n",
      "lin2.weight tensor([[-0.6854,  0.7079,  1.4176, -1.8130, -2.3967],\n",
      "        [ 0.9465, -2.3067, -2.1680,  3.9906,  2.5287]])\n",
      "lin2.bias tensor([ 0.8493, -4.1926])\n",
      "\n",
      "\n",
      "Model: Sequential(\n",
      "  (lin1): Linear(in_features=2, out_features=7, bias=True)\n",
      "  (relu1): LeakyReLU(negative_slope=0.01)\n",
      "  (lin2): Linear(in_features=7, out_features=2, bias=True)\n",
      "  (sigmoid): Hardsigmoid()\n",
      ")\n",
      "Result: [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Expected result: [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: 0.0\n",
      "Accuracy: 1.0\n",
      "lin1.weight tensor([[ 1.1930,  1.1544],\n",
      "        [ 0.6071,  2.3009],\n",
      "        [ 2.0884,  1.3795],\n",
      "        [-3.6774,  0.6787],\n",
      "        [ 2.6053, -3.0984],\n",
      "        [-2.4773, -3.4314],\n",
      "        [-1.5597, -2.1970]])\n",
      "lin1.bias tensor([-1.1879, -0.1903, -1.5146,  2.1276, -0.9238,  2.8153, -2.1254])\n",
      "lin2.weight tensor([[-1.7426,  1.2163, -2.5415,  1.5867,  4.0232, -3.4242,  0.2714],\n",
      "        [ 1.3825,  1.7247,  2.3628, -4.1866, -4.0701, -2.1516, -2.3930]])\n",
      "lin2.bias tensor([-0.0685, -2.1027])\n",
      "\n",
      "\n",
      "Model: Sequential(\n",
      "  (lin1): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (relu1): LeakyReLU(negative_slope=0.01)\n",
      "  (lin2): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (sigmoid): Hardsigmoid()\n",
      ")\n",
      "Result: [[0.48521224 0.        ]\n",
      " [0.5725186  0.04463995]\n",
      " [0.67216945 0.2761825 ]\n",
      " [0.3014524  1.        ]]\n",
      "Expected result: [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: 0.08684840053319931\n",
      "Accuracy: 0.9131515994668007\n",
      "lin1.weight tensor([[ 1.5471,  0.9572],\n",
      "        [ 1.3742,  1.4701],\n",
      "        [ 1.7464,  1.1426],\n",
      "        [-2.3231, -1.9164],\n",
      "        [-0.6883, -0.8341],\n",
      "        [ 1.9922,  1.2677],\n",
      "        [-2.1572, -0.9614],\n",
      "        [ 0.1032,  0.2996],\n",
      "        [ 0.5941,  0.3073],\n",
      "        [-1.6294, -1.9106]])\n",
      "lin1.bias tensor([-2.8769, -1.6088, -3.1997, -2.3639,  1.4151, -0.7666, -2.1346, -1.5183,\n",
      "        -1.3131, -2.9061])\n",
      "lin2.weight tensor([[ 0.5248, -2.7717,  0.7584,  0.4963, -0.1062,  0.8607,  1.1399,  0.2882,\n",
      "          0.6584, -2.1995],\n",
      "        [ 3.3173,  3.0662,  3.7472, -2.1576, -0.9989,  2.0096, -2.1201,  1.2011,\n",
      "          0.5950,  2.2399]])\n",
      "lin2.bias tensor([ 0.0480, -3.0433])\n",
      "\n",
      "\n",
      "Model: Sequential(\n",
      "  (lin1): Linear(in_features=2, out_features=15, bias=True)\n",
      "  (relu1): LeakyReLU(negative_slope=0.01)\n",
      "  (lin2): Linear(in_features=15, out_features=2, bias=True)\n",
      "  (sigmoid): Hardsigmoid()\n",
      ")\n",
      "Result: [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Expected result: [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: 0.0\n",
      "Accuracy: 1.0\n",
      "lin1.weight tensor([[ 2.1715,  0.6451],\n",
      "        [ 1.6613,  2.2715],\n",
      "        [-1.0505, -0.6540],\n",
      "        [-2.5197, -3.0971],\n",
      "        [-2.9945, -2.3514],\n",
      "        [-2.6261, -2.4735],\n",
      "        [-2.8245, -3.1890],\n",
      "        [-2.2377, -2.2252],\n",
      "        [-1.9650,  2.5814],\n",
      "        [ 0.6834,  0.6209],\n",
      "        [ 0.7295, -2.2757],\n",
      "        [-1.8106, -1.8122],\n",
      "        [-3.9325, -1.7946],\n",
      "        [ 2.1865,  1.3064],\n",
      "        [ 1.3466, -2.9441]])\n",
      "lin1.bias tensor([-1.6933, -1.3366,  2.3736, -0.7122, -2.0749, -1.8171,  3.2531, -1.6620,\n",
      "        -1.5890, -1.1238,  1.1683,  0.5835, -2.2223, -1.3386,  0.4849])\n",
      "lin2.weight tensor([[-0.4283, -1.9007,  1.5570,  1.2893, -2.0780,  0.4191, -4.5178,  1.0798,\n",
      "          0.8742, -0.4686,  1.0220,  0.6044,  1.4446, -1.3436,  4.0580],\n",
      "        [ 1.0095,  1.7849, -2.0941, -2.2274, -2.4374, -2.2231, -3.0004, -2.1015,\n",
      "         -1.7563,  0.5376, -0.2628, -1.4152, -2.3795,  0.9323, -2.3415]])\n",
      "lin2.bias tensor([ 2.2061, -2.3566])\n",
      "\n",
      "\n",
      "Model: Sequential(\n",
      "  (lin1): Linear(in_features=2, out_features=5, bias=True)\n",
      "  (relu1): LeakyReLU(negative_slope=0.01)\n",
      "  (lin2): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (relu2): LeakyReLU(negative_slope=0.01)\n",
      "  (lin3): Linear(in_features=5, out_features=2, bias=True)\n",
      "  (sigmoid): Hardsigmoid()\n",
      ")\n",
      "Result: [[0.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.5818082  0.34672412]\n",
      " [0.5956091  0.33435336]]\n",
      "Expected result: [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: 0.13661721348762512\n",
      "Accuracy: 0.8633827865123749\n",
      "lin1.weight tensor([[-2.2781,  1.1812],\n",
      "        [-0.5291, -0.5192],\n",
      "        [-0.5837, -2.7544],\n",
      "        [ 2.4717, -1.0404],\n",
      "        [-1.8856, -0.9995]])\n",
      "lin1.bias tensor([ 0.6905, -1.9869,  2.0131, -0.3346,  0.5921])\n",
      "lin2.weight tensor([[-0.5711, -0.6734, -1.1066, -1.3278, -2.6323],\n",
      "        [ 0.6562, -1.1191,  2.8223, -3.5249,  0.6528],\n",
      "        [-1.3749,  0.9439, -3.2009, -0.3504,  0.7465],\n",
      "        [ 0.1813, -0.4444, -2.5319, -0.4229, -2.7306],\n",
      "        [ 2.9485, -2.0516, -2.5703, -2.4905,  0.6803]])\n",
      "lin2.bias tensor([-0.3454, -1.8758, -0.7929, -2.0164, -1.1498])\n",
      "lin3.weight tensor([[ 1.0823, -1.4615, -1.5089,  0.9010,  1.2956],\n",
      "        [-0.0866, -1.4362,  0.0432,  1.3698, -2.1452]])\n",
      "lin3.bias tensor([ 0.5609, -1.1269])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# Input data.\n",
    "data_in = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]]).float()\n",
    "print(\"Input data:\\n\" + str(data_in) +\"\\n\")\n",
    "\n",
    "# Expected data.\n",
    "data_target = torch.tensor([[0, 0], [1, 0], [1, 0], [0, 1]]).float()\n",
    "print(\"Expected output:\\n\" + str(data_target) + \"\\n\")\n",
    "\n",
    "# The models are here.\n",
    "models = [\n",
    "    nn.Sequential(OrderedDict([\n",
    "        ('lin1', nn.Linear(2, 5)),\n",
    "        ('relu1', nn.LeakyReLU()),\n",
    "        ('lin2', nn.Linear(5, 2)),\n",
    "        ('sigmoid', nn.Hardsigmoid()) \n",
    "    ])),\n",
    "    nn.Sequential(OrderedDict([\n",
    "        ('lin1', nn.Linear(2, 7)),\n",
    "        ('relu1', nn.LeakyReLU()),\n",
    "        ('lin2', nn.Linear(7, 2)),\n",
    "        ('sigmoid', nn.Hardsigmoid()) \n",
    "    ])),\n",
    "    nn.Sequential(OrderedDict([\n",
    "        ('lin1', nn.Linear(2, 10)),\n",
    "        ('relu1', nn.LeakyReLU()),\n",
    "        ('lin2', nn.Linear(10, 2)),\n",
    "        ('sigmoid', nn.Hardsigmoid()) \n",
    "    ])),\n",
    "    nn.Sequential(OrderedDict([\n",
    "        ('lin1', nn.Linear(2, 15)),\n",
    "        ('relu1', nn.LeakyReLU()),\n",
    "        ('lin2', nn.Linear(15, 2)),\n",
    "        ('sigmoid', nn.Hardsigmoid()) \n",
    "    ])),\n",
    "    nn.Sequential(OrderedDict([\n",
    "        ('lin1', nn.Linear(2, 5)),\n",
    "        ('relu1', nn.LeakyReLU()),\n",
    "        ('lin2', nn.Linear(5, 5)),\n",
    "        ('relu2', nn.LeakyReLU()),\n",
    "        ('lin3', nn.Linear(5, 2)),\n",
    "        ('sigmoid', nn.Hardsigmoid()) \n",
    "    ]))\n",
    "]\n",
    "\n",
    "# Optimizers and stuff.\n",
    "criterion = nn.MSELoss()                                                # Loss function\n",
    "optimizers = []                                                         # List of optimizers\n",
    "for model in models:\n",
    "    optimizers.append(torch.optim.Adam(model.parameters(), lr=0.5))\n",
    "\n",
    "# Train the models\n",
    "epoch = 0\n",
    "while True:\n",
    "    epoch = 1 + epoch\n",
    "    maxLoss = 0\n",
    "    minLoss = 1\n",
    "    for model, optimizer in zip(models, optimizers):\n",
    "        output = model(data_in)\n",
    "        loss = criterion(output, data_target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        maxLoss = max(maxLoss, loss.item())\n",
    "        minLoss = min(minLoss, loss.item())\n",
    "    if minLoss == 0:\n",
    "        break\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch: \" + str(epoch) + \", Max Loss: \" + str(maxLoss) + \", Min Loss: \" + str(minLoss))\n",
    "\n",
    "# Print model weights and results.\n",
    "print(\"The best model ended training. It took \" + str(epoch) + \" epochs.\\n\")\n",
    "for model in models:\n",
    "    result = model(data_in)\n",
    "    loss = criterion(result, data_target)\n",
    "    print(\"Model: \" + str(model))\n",
    "    print(\"Result: \" + str(result.detach().numpy()))\n",
    "    print(\"Expected result: \" + str(data_target.numpy()))\n",
    "    print(\"Loss: \" + str(loss.item()))\n",
    "    print(\"Accuracy: \" + str(1 - loss.item()))\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.data)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
